# dota2_agent.py
"""
Dota 2 ReAct Agent
ä½¿ç”¨ ReAct èŒƒå¼ + MCP å·¥å…·è°ƒç”¨
"""

import os
import re
import json
import asyncio
from typing import Optional, Dict, Any, List, AsyncGenerator, Tuple
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥æç¤ºè¯å’Œå·¥å…·å®šä¹‰
from prompts.dota2_agent_prompt import DOTA2_SYSTEM_PROMPT, DOTA2_MCP_TOOLS

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from utils.logger import ConversationLogger

# MCP å®¢æˆ·ç«¯
try:
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
    HAS_MCP = True
except ImportError:
    HAS_MCP = False
    print("âŒ MCP æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install mcp")

# LLM å®¢æˆ·ç«¯
try:
    from openai import OpenAI, AsyncOpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("âŒ OpenAI æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")

# OpenViking å®¢æˆ·ç«¯
try:
    import openviking as ov
    from openviking.message.part import TextPart
    HAS_OPENVIKING = True
except ImportError:
    HAS_OPENVIKING = False
    print("âŒ OpenViking æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openviking")


class Dota2ReActAgent:
    """
    Dota 2 ReAct Agent
    
    ä½¿ç”¨ ReAct (Reasoning + Acting) èŒƒå¼ï¼š
    1. Thought - æ€è€ƒåˆ†æ
    2. Action - è°ƒç”¨ MCP å·¥å…·
    3. Observation - è§‚å¯Ÿç»“æœ
    4. å¾ªç¯æˆ–ç»™å‡º Final Answer
    """
    
    def __init__(
        self,
        mcp_server_path: Optional[str] = None,
        llm_api_key: Optional[str] = None,
        llm_base_url: Optional[str] = None,
        llm_model: Optional[str] = None,
        llm_timeout: float = 150.0,
        max_observation_chars: int = 40000,
        max_iterations: int = 20,
        log_dir: str = "logs",
        enable_logging: bool = True,
        enable_memory: bool = True,
        ov_config_path: Optional[str] = None,
        ov_data_path: Optional[str] = None,
        memory_top_k: int = 3,
        memory_commit_every_n: int = 5,
        memory_commit_min_chars: int = 200,
        memory_commit_only_success: bool = True,
        memory_retrieve_every_n: int = 2,
        memory_retrieve_min_chars: int = 12,
        memory_retrieve_timeout: float = 2.0,
        memory_commit_timeout: float = 1.2,
        memory_record_user_min_chars: int = 8,
        memory_record_assistant_min_chars: int = 80,
    ):
        """
        åˆå§‹åŒ– ReAct Agent
        
        Args:
            mcp_server_path: MCP Server è„šæœ¬è·¯å¾„
            llm_api_key: LLM API Key
            llm_base_url: LLM API Base URL
            llm_model: LLM æ¨¡å‹åç§°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
            enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—
            memory_commit_every_n: è®°å¿†æäº¤é—´éš”ï¼ˆå¯¹è¯æ•°ï¼‰
            memory_commit_min_chars: è®°å¿†æäº¤æœ€å°å­—æ•°
            memory_commit_only_success: ä»…æˆåŠŸæ—¶æäº¤è®°å¿†
            memory_retrieve_every_n: è®°å¿†æ£€ç´¢é—´éš”ï¼ˆå¯¹è¯æ•°ï¼‰
            memory_retrieve_min_chars: è§¦å‘è®°å¿†æ£€ç´¢æœ€å°å­—æ•°
            memory_retrieve_timeout: è®°å¿†æ£€ç´¢è¶…æ—¶ï¼ˆç§’ï¼‰
            memory_commit_timeout: è®°å¿†æäº¤è¶…æ—¶ï¼ˆç§’ï¼‰
            memory_record_user_min_chars: è®°å½•ç”¨æˆ·æœ€å°å­—æ•°
            memory_record_assistant_min_chars: è®°å½•åŠ©æ‰‹æœ€å°å­—æ•°
        """
        # MCP é…ç½®
        self.mcp_server_path = mcp_server_path or os.path.join(
            os.path.dirname(__file__),
            "mcp_server",
            "dota2_fastmcp.py"
        )
        self.session: Optional[ClientSession] = None
        self.mcp_tools: List[Dict] = []
        
        # LLM é…ç½®
        self.llm_api_key = llm_api_key or os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
        self.llm_base_url = llm_base_url or os.getenv("OPENAI_BASE_URL") or os.getenv("LLM_BASE_URL")
        self.llm_model = llm_model or os.getenv("LLM_MODEL_ID") or "deepseek-v3.2"
        self.llm_timeout = float(os.getenv("LLM_TIMEOUT", llm_timeout))
        self.max_observation_chars = int(os.getenv("MAX_OBSERVATION_CHARS", max_observation_chars))
        
        self.max_iterations = max_iterations
        self.system_prompt = DOTA2_SYSTEM_PROMPT
        
        # æ—¥å¿—
        self.enable_logging = enable_logging
        self.log_dir = log_dir
        self.logger = ConversationLogger(log_dir) if enable_logging else None

        # OpenViking è®°å¿†é…ç½®
        self.enable_memory = enable_memory
        self.ov_config_path = ov_config_path or os.path.join(os.path.dirname(__file__), "ov.conf")
        self.ov_data_path = ov_data_path or os.path.join(os.path.dirname(__file__), "ov_data")
        self.memory_top_k = memory_top_k
        self.memory_commit_every_n = max(1, int(memory_commit_every_n))
        self.memory_commit_min_chars = max(0, int(memory_commit_min_chars))
        self.memory_commit_only_success = bool(memory_commit_only_success)
        self.memory_retrieve_every_n = max(1, int(memory_retrieve_every_n))
        self.memory_retrieve_min_chars = max(0, int(memory_retrieve_min_chars))
        self.memory_retrieve_timeout = max(0.2, float(memory_retrieve_timeout))
        self.memory_commit_timeout = max(0.2, float(memory_commit_timeout))
        self.memory_record_user_min_chars = max(0, int(memory_record_user_min_chars))
        self.memory_record_assistant_min_chars = max(0, int(memory_record_assistant_min_chars))
        self.ov_client = None
        self.ov_session = None
        self.ov_session_id = self.logger.session_id if self.logger else None
        self._memory_pending_count = 0
        self._memory_commit_lock = asyncio.Lock()
        self._memory_turn = 0
        self._memory_last_query = ""
        self._memory_last_context = ""
        self._last_user_input = ""
        self._pending_visual_markdown: List[str] = []
        self._last_assistant_answer = ""
        self._recent_turns: List[Dict[str, str]] = []
        self._background_tasks: set = set()
        
        # LLM å®¢æˆ·ç«¯
        if HAS_OPENAI and self.llm_api_key:
            self.llm_client = OpenAI(
                api_key=self.llm_api_key,
                base_url=self.llm_base_url,
                max_retries=0,
            )
            try:
                self.llm_async_client = AsyncOpenAI(
                    api_key=self.llm_api_key,
                    base_url=self.llm_base_url,
                    max_retries=0,
                )
            except Exception:
                self.llm_async_client = None
        else:
            self.llm_client = None
            self.llm_async_client = None

    async def _ensure_memory_ready(self) -> bool:
        """åˆå§‹åŒ– OpenViking è®°å¿†å­˜å‚¨ï¼ˆåªåšä¸€æ¬¡ï¼‰"""
        if not self.enable_memory or not HAS_OPENVIKING:
            return False
        if self.ov_client and self.ov_session:
            return True
        try:
            if not (self.ov_config_path and os.path.exists(self.ov_config_path)):
                print("âš ï¸ OpenViking é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²å…³é—­è®°å¿†åŠŸèƒ½")
                self.enable_memory = False
                return False

            os.environ["OPENVIKING_CONFIG_FILE"] = os.path.abspath(self.ov_config_path)
            os.makedirs(self.ov_data_path, exist_ok=True)

            self.ov_client = ov.AsyncOpenViking(path=self.ov_data_path)
            await self.ov_client.initialize()

            if not self.ov_session_id and self.logger:
                self.ov_session_id = self.logger.session_id
            self.ov_session = self.ov_client.session(session_id=self.ov_session_id)
            await asyncio.to_thread(self.ov_session.load)
            return True
        except Exception as e:
            print(f"âš ï¸ OpenViking åˆå§‹åŒ–å¤±è´¥ï¼Œå·²å…³é—­è®°å¿†åŠŸèƒ½: {e}")
            self.enable_memory = False
            return False

    async def _record_memory_message(self, role: str, content: str, session: Optional[Any] = None) -> None:
        target_session = session or self.ov_session
        if not target_session or not content:
            return

        def _add():
            target_session.add_message(role, [TextPart(text=content)])

        await asyncio.to_thread(_add)

    async def _commit_memory_session(self, session: Optional[Any] = None) -> None:
        target_session = session or self.ov_session
        if not target_session:
            return
        async with self._memory_commit_lock:
            await asyncio.to_thread(target_session.commit)

    def _build_memory_query(self, user_input: str) -> tuple[str, bool]:
        cleaned = user_input.strip()
        if len(cleaned) >= self.memory_retrieve_min_chars:
            return cleaned, False
        parts = [p for p in (self._last_user_input, self._last_assistant_answer, cleaned) if p]
        if parts:
            return "\n".join(parts), True
        return cleaned, False

    def _needs_recent_context(self, user_input: str) -> bool:
        if not user_input:
            return False
        cleaned = user_input.strip()
        if len(cleaned) <= 14:
            return True
        pronouns = ("ä»–ä»¬", "å¥¹ä»¬", "ä»–ä»¬çš„", "å¥¹ä»¬çš„", "ä»–", "å¥¹", "å®ƒ", "è¿™äº›", "é‚£äº›", "è¿™ä¸ª", "é‚£ä¸ª", "ä¸Šè¿°", "ä¸Šé¢", "ä¹‹å‰", "åˆšæ‰", "é‚£æ¬¡", "é‚£ä»–ä»¬", "ä»–ä»¬æœ€è¿‘")
        return any(p in cleaned for p in pronouns)

    def _build_recent_context(self) -> str:
        if not self._recent_turns:
            return ""
        def _clip(text: str, limit: int = 800) -> str:
            if not text:
                return ""
            return text if len(text) <= limit else text[:limit] + "â€¦"
        lines = ["æœ€è¿‘å¯¹è¯æ‘˜è¦ï¼ˆç”¨äºæŒ‡ä»£æ¶ˆè§£ï¼‰ï¼š"]
        for idx, turn in enumerate(self._recent_turns[-3:], start=1):
            user_text = _clip(turn.get("user", ""))
            assistant_text = _clip(turn.get("assistant", ""))
            if user_text:
                lines.append(f"- ç¬¬{idx}è½®ç”¨æˆ·ï¼š{user_text}")
            if assistant_text:
                lines.append(f"- ç¬¬{idx}è½®åŠ©æ‰‹ï¼š{assistant_text}")
        return "\n".join(lines)

    def _record_recent_turn(self, user_input: str, assistant_answer: str) -> None:
        if not (user_input or assistant_answer):
            return
        self._recent_turns.append({
            "user": user_input,
            "assistant": assistant_answer,
        })
        if len(self._recent_turns) > 20:
            self._recent_turns = self._recent_turns[-20:]

    def load_recent_context_from_session(self, conversations: List[Dict[str, Any]]) -> None:
        """ä»å†å²ä¼šè¯åŠ è½½æœ€è¿‘2-3è½®ä¸Šä¸‹æ–‡ã€‚"""
        turns: List[Dict[str, str]] = []
        for conv in conversations or []:
            user_text = str(conv.get("user_input") or "").strip()
            assistant_text = str(conv.get("final_answer") or "").strip()
            if not (user_text or assistant_text):
                continue
            turns.append({"user": user_text, "assistant": assistant_text})
        if not turns:
            return
        self._recent_turns = turns
        last = self._recent_turns[-1]
        self._last_user_input = last.get("user", "")
        self._last_assistant_answer = last.get("assistant", "")

    def _should_retrieve_memory(self, query: str, force: bool) -> bool:
        if not self.enable_memory or not HAS_OPENVIKING:
            return False
        if not query or len(query.strip()) < self.memory_retrieve_min_chars:
            return False
        if force:
            return True
        self._memory_turn += 1
        if self._memory_turn % self.memory_retrieve_every_n != 1:
            return False
        return True

    async def _maybe_commit_recent_memory(self, force: bool) -> None:
        if not force or not self.ov_session or self._memory_pending_count <= 0:
            return
        try:
            await asyncio.wait_for(
                self._commit_memory_session(session=self.ov_session),
                timeout=self.memory_commit_timeout,
            )
            self._memory_pending_count = 0
        except asyncio.TimeoutError:
            return

    async def _retrieve_memory_context(self, query: str) -> str:
        if not self.ov_client or not query:
            return ""
        if query == self._memory_last_query and self._memory_last_context:
            return self._memory_last_context
        try:
            result = await asyncio.wait_for(
                self.ov_client.search(
                    query=query,
                    target_uri="viking://user/memories",
                    session=self.ov_session,
                    limit=self.memory_top_k,
                ),
                timeout=self.memory_retrieve_timeout,
            )
            memories = result.memories if result else []
            if not memories:
                self._memory_last_query = query
                self._memory_last_context = ""
                return ""

            try:
                self.ov_session.used(contexts=[m.uri for m in memories])
            except Exception:
                pass

            lines = []
            for mem in memories[: self.memory_top_k]:
                summary = mem.abstract or ""
                if mem.overview and mem.overview != summary:
                    summary = f"{summary}ï¼ˆè¯¦æƒ…ï¼š{mem.overview}ï¼‰" if summary else mem.overview
                if mem.category:
                    summary = f"[{mem.category}] {summary}" if summary else f"[{mem.category}]"
                if mem.match_reason:
                    summary = f"{summary}ï¼ˆåŒ¹é…åŸå› ï¼š{mem.match_reason}ï¼‰"
                if summary:
                    lines.append(f"- {summary}")

            if not lines:
                self._memory_last_query = query
                self._memory_last_context = ""
                return ""

            context = "ç›¸å…³è®°å¿†ï¼ˆä¾›å‚è€ƒï¼Œå¯èƒ½ä¸å®Œæ•´ï¼‰ï¼š\n" + "\n".join(lines)
            self._memory_last_query = query
            self._memory_last_context = context
            return context
        except asyncio.TimeoutError:
            return ""
        except Exception as e:
            print(f"âš ï¸ è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
            return ""

    async def _finalize_memory(
        self,
        assistant_text: str,
        status: str = "success",
        force_commit: bool = False,
        session: Optional[Any] = None,
    ) -> None:
        target_session = session or self.ov_session
        if not target_session:
            return

        if assistant_text and len(assistant_text.strip()) >= self.memory_record_assistant_min_chars:
            await self._record_memory_message("assistant", assistant_text, session=target_session)
            if target_session is self.ov_session:
                self._memory_pending_count += 1

        if not force_commit:
            if self.memory_commit_only_success and status != "success":
                return
            if assistant_text and len(assistant_text.strip()) < self.memory_commit_min_chars:
                return
            if target_session is self.ov_session and self._memory_pending_count < self.memory_commit_every_n:
                return

        await self._commit_memory_session(session=target_session)
        if target_session is self.ov_session:
            self._memory_pending_count = 0

    def _finalize_memory_background(
        self,
        assistant_text: str,
        status: str = "success",
        force_commit: bool = False,
        session: Optional[Any] = None,
    ) -> None:
        """åå°æäº¤è®°å¿†ï¼Œé¿å…é˜»å¡å‰ç«¯å“åº”ã€‚"""
        target_session = session or self.ov_session
        if not target_session:
            return
        task = asyncio.create_task(
            self._finalize_memory(
                assistant_text,
                status=status,
                force_commit=force_commit,
                session=target_session,
            )
        )
        self._background_tasks.add(task)
        task.add_done_callback(self._background_tasks.discard)
    
    async def connect_mcp(self):
        """è¿æ¥åˆ° MCP Server"""
        if not HAS_MCP:
            raise RuntimeError("MCP æœªå®‰è£…")
        
        server_params = StdioServerParameters(
            command="python",
            args=[self.mcp_server_path],
        )
        
        self._stdio_client = stdio_client(server_params)
        self._read, self._write = await self._stdio_client.__aenter__()
        
        self.session = ClientSession(self._read, self._write)
        await self.session.__aenter__()
        await self.session.initialize()
        
        # è·å–å¯ç”¨å·¥å…·
        tools_result = await self.session.list_tools()
        self.mcp_tools = [
            {
                "name": tool.name,
                "description": tool.description,
                "inputSchema": tool.inputSchema
            }
            for tool in tools_result.tools
        ]
        
        print(f"âœ… MCP è¿æ¥æˆåŠŸï¼Œå¯ç”¨å·¥å…·: {[t['name'] for t in self.mcp_tools]}")
    
    async def disconnect_mcp(self):
        """æ–­å¼€ MCP è¿æ¥"""
        # å°½é‡å¿«é€Ÿå–æ¶ˆåå°ä»»åŠ¡ï¼Œé¿å…é˜»å¡é€€å‡º
        if self._background_tasks:
            for task in list(self._background_tasks):
                task.cancel()
            await asyncio.gather(*self._background_tasks, return_exceptions=True)
            self._background_tasks.clear()

        if self.ov_client:
            if self.ov_session and self._memory_pending_count > 0:
                try:
                    await asyncio.wait_for(
                        self._finalize_memory("", force_commit=True),
                        timeout=self.memory_commit_timeout,
                    )
                except asyncio.TimeoutError:
                    pass
            try:
                await asyncio.wait_for(self.ov_client.close(), timeout=1.5)
            except asyncio.TimeoutError:
                pass
            self.ov_client = None
            self.ov_session = None

        if self.session:
            try:
                await asyncio.wait_for(self.session.__aexit__(None, None, None), timeout=1.5)
            except asyncio.TimeoutError:
                pass
        if hasattr(self, '_stdio_client'):
            try:
                await asyncio.wait_for(self._stdio_client.__aexit__(None, None, None), timeout=1.5)
            except asyncio.TimeoutError:
                pass
        
        # ä¿å­˜ä¼šè¯æ—¥å¿—
        if self.logger:
            self.logger.save_session()

    async def start_new_session(self) -> None:
        """åˆ›å»ºä¸€ä¸ªæ–°çš„æ—¥å¿—/è®°å¿†ä¼šè¯"""
        old_session = self.ov_session
        had_pending = self._memory_pending_count > 0
        if self.logger:
            self.logger.save_session()
            self.logger = ConversationLogger(self.log_dir)

        self.ov_session_id = self.logger.session_id if self.logger else None
        self.ov_session = None
        self._memory_pending_count = 0
        self._memory_turn = 0
        self._memory_last_query = ""
        self._memory_last_context = ""
        self._last_user_input = ""
        self._last_assistant_answer = ""
        self._recent_turns = []

        if self.enable_memory and self.ov_client and self.ov_session_id:
            self.ov_session = self.ov_client.session(session_id=self.ov_session_id)
            await asyncio.to_thread(self.ov_session.load)

        if old_session and had_pending:
            self._finalize_memory_background("", force_commit=True, session=old_session)
    
    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:
        """è°ƒç”¨ MCP å·¥å…·"""
        if not self.session:
            raise RuntimeError("MCP æœªè¿æ¥")
        
        result = await self.session.call_tool(tool_name, arguments)
        
        if result.content:
            return result.content[0].text
        return "æ— ç»“æœ"
    
    def _call_llm(self, messages: List[Dict]) -> str:
        """è°ƒç”¨ LLM"""
        if not self.llm_client:
            raise RuntimeError("LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·è®¾ç½® API Key")
        
        response = self.llm_client.chat.completions.create(
            model=self.llm_model,
            messages=messages,
            temperature=0.7,
            timeout=self.llm_timeout,
        )
        
        return response.choices[0].message.content

    async def _call_llm_async(self, messages: List[Dict]) -> str:
        """å¼‚æ­¥è°ƒç”¨ LLMï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰"""
        if self.llm_async_client:
            response = await self.llm_async_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                temperature=0.7,
                timeout=self.llm_timeout,
            )
            return response.choices[0].message.content
        return await asyncio.wait_for(
            asyncio.to_thread(self._call_llm, messages),
            timeout=self.llm_timeout,
        )

    async def _call_llm_stream(self, messages: List[Dict]) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨ LLMï¼ˆé€æ®µäº§å‡ºå†…å®¹ï¼‰"""
        if self.llm_async_client:
            stream = await self.llm_async_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                temperature=0.7,
                timeout=self.llm_timeout,
                stream=True,
            )
            async for chunk in stream:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    yield delta.content
            return
        response = await self._call_llm_async(messages)
        if response:
            yield response
    
    def _parse_action(self, response: str) -> Optional[Dict]:
        """
        è§£æ LLM å“åº”ä¸­çš„ Action
        
        Returns:
            {"action": "tool_name", "action_input": {...}} æˆ– None
        """
        # åŒ¹é… Action å’Œ Action Input
        action_match = re.search(r'Action:\s*(\w+)', response)
        input_match = re.search(r'Action Input:\s*(\{.*?\})', response, re.DOTALL)
        
        if action_match:
            action = action_match.group(1)
            action_input = {}
            
            if input_match:
                try:
                    action_input = json.loads(input_match.group(1))
                except json.JSONDecodeError:
                    pass
            
            return {"action": action, "action_input": action_input}
        
        return None
    
    def _clean_llm_response(self, response: str) -> str:
        """
        æ¸…ç† LLM å“åº”ï¼Œç§»é™¤ LLM è‡ªå·±ç”Ÿæˆçš„å‡ Observation
        
        æœ‰äº› LLM ä¼šè‡ªå·±å¹»è§‰ç”Ÿæˆ Observationï¼Œéœ€è¦å°†å…¶ç§»é™¤
        """
        # æ£€æµ‹å¹¶ç§»é™¤ LLM è‡ªå·±ç”Ÿæˆçš„ Observation åŠå…¶åé¢çš„å†…å®¹
        # åªä¿ç•™ Action Input ä¹‹å‰ï¼ˆåŒ…æ‹¬ï¼‰çš„å†…å®¹
        
        # æŸ¥æ‰¾ Action Input çš„ä½ç½®
        input_match = re.search(r'Action Input:\s*\{[^}]*\}', response, re.DOTALL)
        if input_match:
            # è§„èŒƒåŒ–ï¼šAction Input åä¸å…è®¸ç»§ç»­è¾“å‡º
            after_input = response[input_match.end():]
            if after_input.strip():
                print("âš ï¸ æ£€æµ‹åˆ° Action Input åç»§ç»­è¾“å‡ºï¼Œå·²æˆªæ–­")
                return response[:input_match.end()]
        
        return response

    def _trim_observation_for_llm(self, observation: str) -> str:
        if not observation:
            return observation
        # å‡å°‘æ— æ„ä¹‰ç©ºç™½ï¼Œé™ä½ä¸Šä¸‹æ–‡å™ªå£°
        observation = observation.replace("\n", " ").replace("\"", "")
        observation = re.sub(r"[ \t]+", " ", observation)
        if self.max_observation_chars <= 0:
            return observation
        if len(observation) <= self.max_observation_chars:
            return observation
        truncated = observation[: self.max_observation_chars]
        return truncated + "\n...[truncated for context limit]"
    
    def _parse_final_answer(self, response: str) -> Optional[str]:
        """è§£ææœ€ç»ˆç­”æ¡ˆ"""
        match = re.search(r'Final Answer:\s*(.*)', response, re.DOTALL)
        if match:
            return match.group(1).strip()
        return None

    def _extract_thought(self, response: str) -> Optional[str]:
        match = re.search(r'Thought:\s*(.*?)(?:\nAction:|\nFinal Answer:|\Z)', response, re.DOTALL)
        if match:
            thought = match.group(1).strip()
            return thought or None
        return None

    def _extract_ward_html_path(self, text: str) -> Optional[str]:
        match = re.search(r'(ward_analysis[\\/](ward_(?:timeline|multi)_[^\\/]+\.html))', text)
        if match:
            return "/" + match.group(1).replace("\\", "/")
        return None

    def _reset_visual_reports(self) -> None:
        self._pending_visual_markdown = []

    def _maybe_capture_visual_report(self, tool_name: str, observation: str) -> None:
        if tool_name != "save_match_details_report":
            return
        marker_line = ""
        lines = observation.splitlines()
        marker_index = None
        for idx, line in enumerate(lines):
            if "Markdown" in line and line.strip().startswith("##"):
                marker_line = line.strip()
                marker_index = idx
                break
        if marker_index is None:
            return
        report_body = "\n".join(lines[marker_index + 1:]).strip()
        if not report_body:
            return
        report_markdown = f"{marker_line}\n{report_body}".strip()
        if report_markdown not in self._pending_visual_markdown:
            self._pending_visual_markdown.append(report_markdown)

    def _append_visual_reports(self, final_answer: str) -> Tuple[str, str]:
        if not self._pending_visual_markdown:
            return final_answer, ""
        appended: List[str] = []
        for report in self._pending_visual_markdown:
            report_text = (report or "").strip()
            if not report_text:
                continue
            if report_text in final_answer:
                continue
            first_line = next((line for line in report_text.splitlines() if line.strip()), "")
            if first_line and first_line in final_answer:
                continue
            appended.append(report_text)
        if not appended:
            self._pending_visual_markdown = []
            return final_answer, ""
        separator = "\n\n" if final_answer.strip() else ""
        appended_text = separator + "\n\n".join(appended)
        self._pending_visual_markdown = []
        return f"{final_answer}{appended_text}", appended_text
    
    async def run(self, user_input: str) -> str:
        """
        æ‰§è¡Œ ReAct å¾ªç¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            æœ€ç»ˆå›ç­”
        """
        if not self.llm_client:
            raise RuntimeError("LLM æœªé…ç½®ï¼Œæ— æ³•è¿è¡Œ ReAct æ¨¡å¼")

        memory_context = ""
        memory_ready = await self._ensure_memory_ready() if self.enable_memory else False
        memory_query, force_retrieve = self._build_memory_query(user_input)
        if self._should_retrieve_memory(memory_query, force_retrieve) and memory_ready:
            await self._maybe_commit_recent_memory(force_retrieve)
            memory_context = await self._retrieve_memory_context(memory_query)
        if memory_ready and len(user_input.strip()) >= self.memory_record_user_min_chars:
            await self._record_memory_message("user", user_input)

        self._reset_visual_reports()

        # å¼€å§‹è®°å½•å¯¹è¯
        if self.logger:
            self.logger.start_conversation(user_input, self.llm_model)
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "system", "content": self.system_prompt}]
        if memory_context:
            messages.append({"role": "system", "content": memory_context})
        if self._needs_recent_context(user_input):
            recent_context = self._build_recent_context()
            if recent_context:
                messages.append({"role": "system", "content": recent_context})
        messages.append({"role": "user", "content": user_input})
        
        try:
            for i in range(self.max_iterations):
                # è°ƒç”¨ LLMï¼ˆå¼‚æ­¥ï¼Œé¿å…é˜»å¡ï¼‰
                print("\nâ³ æ­£åœ¨è¯·æ±‚ LLM...")
                try:
                    response = await asyncio.wait_for(
                        self._call_llm_async(messages),
                        timeout=self.llm_timeout,
                    )
                except asyncio.TimeoutError:
                    result = f"LLM è¯·æ±‚è¶…æ—¶ï¼ˆ>{self.llm_timeout:.0f}sï¼‰ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                    if self.logger:
                        self.logger.end_conversation(result, "timeout")
                    self._last_user_input = user_input
                    self._last_assistant_answer = result
                    self._record_recent_turn(user_input, result)
                    self._finalize_memory_background(result, status="timeout")
                    return result
                except Exception as e:
                    result = f"LLM è°ƒç”¨å¤±è´¥: {e}"
                    if self.logger:
                        self.logger.end_conversation(result, "error")
                    self._last_user_input = user_input
                    self._last_assistant_answer = result
                    self._record_recent_turn(user_input, result)
                    self._finalize_memory_background(result, status="error")
                    return result
                
                # æ¸…ç† LLM å“åº”ï¼Œç§»é™¤è‡ªå·±ç”Ÿæˆçš„å‡ Observation
                response = self._clean_llm_response(response)
                
                print(f"\n--- è¿­ä»£ {i+1} ---")
                print(response)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
                final_answer = self._parse_final_answer(response)
                if final_answer:
                    final_answer, _ = self._append_visual_reports(final_answer)
                    # è®°å½•æœ€åä¸€æ¬¡è¿­ä»£
                    if self.logger:
                        self.logger.log_iteration(i + 1, response)
                        self.logger.end_conversation(final_answer, "success")
                    self._last_user_input = user_input
                    self._last_assistant_answer = final_answer
                    self._record_recent_turn(user_input, final_answer)
                    self._finalize_memory_background(final_answer, status="success")
                    return final_answer
                
                # è§£æ Action
                action_data = self._parse_action(response)
                
                if action_data:
                    tool_name = action_data["action"]
                    tool_input = action_data["action_input"]
                    
                    print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}")
                    print(f"   å‚æ•°: {tool_input}")
                    
                    # è°ƒç”¨ MCP å·¥å…·
                    try:
                        observation = await self.call_mcp_tool(tool_name, tool_input)
                    except Exception as e:
                        observation = f"å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}"
                    
                    print(f"\nğŸ“‹ Observation:\n{observation[:500]}...")
                    self._maybe_capture_visual_report(tool_name, observation)
                    
                    # è®°å½•è¿­ä»£
                    if self.logger:
                        self.logger.log_iteration(
                            i + 1, response,
                            action=tool_name,
                            action_input=tool_input,
                            observation=observation
                        )
                    
                    # å°†ç»“æœåŠ å…¥æ¶ˆæ¯å†å²ï¼ˆé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
                    observation_for_llm = self._trim_observation_for_llm(observation)
                    messages.append({"role": "assistant", "content": response})
                    messages.append({"role": "user", "content": f"Observation: {observation_for_llm}"})
                else:
                    # æ²¡æœ‰ Action ä¹Ÿæ²¡æœ‰ Final Answerï¼Œå¯èƒ½æ˜¯æ ¼å¼é—®é¢˜
                    if self.logger:
                        self.logger.log_iteration(i + 1, response)
                    
                    messages.append({"role": "assistant", "content": response})
                    messages.append({
                        "role": "user",
                        "content": "è¯·æŒ‰ç…§ ReAct æ ¼å¼å›å¤ï¼šä½¿ç”¨ Action/Action Input è°ƒç”¨å·¥å…·ï¼Œæˆ–ä½¿ç”¨ Final Answer ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"
                    })
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            result = "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ— æ³•å®Œæˆä»»åŠ¡ã€‚"
            if self.logger:
                self.logger.end_conversation(result, "max_iterations")
            self._last_user_input = user_input
            self._last_assistant_answer = result
            self._record_recent_turn(user_input, result)
            self._finalize_memory_background(result, status="max_iterations")
            return result
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            if self.logger:
                self.logger.end_conversation(str(e), "error")
            self._last_user_input = user_input
            self._last_assistant_answer = str(e)
            self._record_recent_turn(user_input, str(e))
            self._finalize_memory_background(str(e), status="error")
            raise

    async def run_stream(self, user_input: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æ‰§è¡Œ ReAct å¾ªç¯ï¼ˆæµå¼è¾“å‡º Thought/Action/Observation ä¸æœ€ç»ˆç­”æ¡ˆï¼‰
        """
        if not self.llm_client:
            raise RuntimeError("LLM æœªé…ç½®ï¼Œæ— æ³•è¿è¡Œ ReAct æ¨¡å¼")

        memory_context = ""
        memory_ready = await self._ensure_memory_ready() if self.enable_memory else False
        memory_query, force_retrieve = self._build_memory_query(user_input)
        if self._should_retrieve_memory(memory_query, force_retrieve) and memory_ready:
            await self._maybe_commit_recent_memory(force_retrieve)
            memory_context = await self._retrieve_memory_context(memory_query)
        if memory_ready and len(user_input.strip()) >= self.memory_record_user_min_chars:
            await self._record_memory_message("user", user_input)

        self._reset_visual_reports()

        if self.logger:
            self.logger.start_conversation(user_input, self.llm_model)
            if self.logger.current_conversation:
                yield {
                    "type": "session",
                    "session_id": self.logger.session_id,
                    "conversation_id": self.logger.current_conversation.get("id"),
                    "timestamp": self.logger.current_conversation.get("timestamp"),
                    "status": self.logger.current_conversation.get("status", "running"),
                }

        messages = [
            {"role": "system", "content": self.system_prompt},
        ]
        if memory_context:
            messages.append({"role": "system", "content": memory_context})
        if self._needs_recent_context(user_input):
            recent_context = self._build_recent_context()
            if recent_context:
                messages.append({"role": "system", "content": recent_context})
        messages.append({"role": "user", "content": user_input})

        ward_html = None

        try:
            for i in range(self.max_iterations):
                print("\nâ³ æ­£åœ¨è¯·æ±‚ LLM...")
                try:
                    response = ""

                    async for chunk in self._call_llm_stream(messages):
                        response += chunk
                except asyncio.TimeoutError:
                    result = f"LLM è¯·æ±‚è¶…æ—¶ï¼ˆ>{self.llm_timeout:.0f}sï¼‰ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                    if self.logger:
                        self.logger.end_conversation(result, "timeout")
                    yield {"type": "final", "content": result, "ward_html": ward_html}
                    self._last_user_input = user_input
                    self._last_assistant_answer = result
                    self._record_recent_turn(user_input, result)
                    self._finalize_memory_background(result, status="timeout")
                    return
                except Exception as e:
                    result = f"LLM è°ƒç”¨å¤±è´¥: {e}"
                    if self.logger:
                        self.logger.end_conversation(result, "error")
                    yield {"type": "final", "content": result, "ward_html": ward_html}
                    self._last_user_input = user_input
                    self._last_assistant_answer = result
                    self._record_recent_turn(user_input, result)
                    self._finalize_memory_background(result, status="error")
                    return
                if not response:
                    result = "LLM è¿”å›ä¸ºç©ºï¼Œè¯·ç¨åé‡è¯•ã€‚"
                    if self.logger:
                        self.logger.end_conversation(result, "error")
                    yield {"type": "final", "content": result, "ward_html": ward_html}
                    self._last_user_input = user_input
                    self._last_assistant_answer = result
                    self._record_recent_turn(user_input, result)
                    self._finalize_memory_background(result, status="error")
                    return
                response = self._clean_llm_response(response)

                print(f"\n--- è¿­ä»£ {i+1} ---")
                print(response)

                thought = self._extract_thought(response)
                if thought:
                    yield {"type": "thought", "content": thought}

                final_answer = self._parse_final_answer(response)
                if final_answer:
                    final_answer, appended_text = self._append_visual_reports(final_answer)
                    if appended_text:
                        yield {"type": "final_delta", "content": appended_text}
                    if self.logger:
                        self.logger.log_iteration(i + 1, response)
                        self.logger.end_conversation(final_answer, "success")
                    yield {"type": "final", "content": final_answer, "ward_html": ward_html}
                    self._last_user_input = user_input
                    self._last_assistant_answer = final_answer
                    self._record_recent_turn(user_input, final_answer)
                    self._finalize_memory_background(final_answer, status="success")
                    return

                action_data = self._parse_action(response)

                if action_data:
                    tool_name = action_data["action"]
                    tool_input = action_data["action_input"]

                    yield {"type": "action", "content": tool_name, "input": tool_input}

                    print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}")
                    print(f"   å‚æ•°: {tool_input}")

                    try:
                        observation = await self.call_mcp_tool(tool_name, tool_input)
                    except Exception as e:
                        observation = f"å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}"

                    if tool_name in ("analyze_match_wards", "analyze_multi_match_wards", "inject_multi_match_ward_report_html"):
                        ward_html = self._extract_ward_html_path(observation) or ward_html

                    print(f"\nğŸ“‹ Observation:\n{observation[:500]}...")
                    self._maybe_capture_visual_report(tool_name, observation)

                    yield {"type": "observation", "content": observation}

                    if self.logger:
                        self.logger.log_iteration(
                            i + 1, response,
                            action=tool_name,
                            action_input=tool_input,
                            observation=observation
                        )

                    observation_for_llm = self._trim_observation_for_llm(observation)
                    messages.append({"role": "assistant", "content": response})
                    messages.append({"role": "user", "content": f"Observation: {observation_for_llm}"})
                else:
                    if self.logger:
                        self.logger.log_iteration(i + 1, response)

                    messages.append({"role": "assistant", "content": response})
                    messages.append({
                        "role": "user",
                        "content": "è¯·æŒ‰ç…§ ReAct æ ¼å¼å›å¤ï¼šä½¿ç”¨ Action/Action Input è°ƒç”¨å·¥å…·ï¼Œæˆ–ä½¿ç”¨ Final Answer ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"
                    })

            result = "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ— æ³•å®Œæˆä»»åŠ¡ã€‚"
            if self.logger:
                self.logger.end_conversation(result, "max_iterations")
            yield {"type": "final", "content": result, "ward_html": ward_html}
            self._last_user_input = user_input
            self._last_assistant_answer = result
            self._record_recent_turn(user_input, result)
            self._finalize_memory_background(result, status="max_iterations")

        except Exception as e:
            if self.logger:
                self.logger.end_conversation(str(e), "error")
            self._last_user_input = user_input
            self._last_assistant_answer = str(e)
            self._record_recent_turn(user_input, str(e))
            self._finalize_memory_background(str(e), status="error")
            raise


# ==================== ä¸»å‡½æ•° ====================

async def main():
    """ä¸»å…¥å£"""
    print("=" * 60)
    print("  ğŸ® Dota 2 ReAct Agent")
    print("  (ReAct èŒƒå¼ + MCP å·¥å…·è°ƒç”¨)")
    print("=" * 60)
    print()
    
    agent = Dota2ReActAgent(enable_logging=True)
    
    # æ˜¾ç¤º LLM é…ç½®ä¿¡æ¯
    print("ğŸ“Œ LLM é…ç½®:")
    if agent.llm_client:
        print(f"   æ¨¡å‹: {agent.llm_model}")
        print(f"   API: {agent.llm_base_url or 'OpenAI é»˜è®¤'}")
        print(f"   çŠ¶æ€: âœ… å·²è¿æ¥")
    else:
        print(f"   çŠ¶æ€: âŒ æœªé…ç½®")
        print(f"   æç¤º: è®¾ç½® LLM_API_KEY å’Œ LLM_BASE_URL ç¯å¢ƒå˜é‡")
        print("\nâŒ LLM æœªé…ç½®ï¼Œæ— æ³•å¯åŠ¨ ReAct Agent")
        return
    
    # æ˜¾ç¤ºæ—¥å¿—é…ç½®
    if agent.logger:
        print(f"\nğŸ“Œ æ—¥å¿—é…ç½®:")
        print(f"   ç›®å½•: {agent.logger.log_dir}/")
        print(f"   ä¼šè¯: {agent.logger.session_id}")
    print()
    
    try:
        await agent.connect_mcp()
        
        # æ˜¾ç¤º MCP å·¥å…·ä¿¡æ¯
        print(f"ğŸ“Œ MCP å·¥å…·: {len(agent.mcp_tools)} ä¸ªå¯ç”¨")
        print("\nè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")
        
        while True:
            try:
                user_input = input("ä½ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("å†è§ï¼")
                    break
                
                if not user_input:
                    continue
                
                # æ‰§è¡Œ ReAct å¾ªç¯
                response = await agent.run(user_input)
                print(f"\n{'='*60}")
                print(f"âœ… æœ€ç»ˆå›ç­”:\n{response}")
                print(f"{'='*60}\n")
                
            except KeyboardInterrupt:
                print("\nå†è§ï¼")
                break
            except Exception as e:
                print(f"é”™è¯¯: {e}\n")
    
    finally:
        await agent.disconnect_mcp()


if __name__ == "__main__":
    asyncio.run(main())
